% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!



%%% This is all Custom
\usepackage{amsmath}
\DeclareMathOperator{\interp}{interp}
\DeclareMathOperator{\QR}{QR}
\DeclareMathOperator{\sd}{sd}
\newcommand{\mat}{\mathbf}

%%% END Article customizations

%%% The "real" document content comes below...

\title{Singular values of covariance matrices under localization}
\author{Travis Harty}
\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed

\begin{document}
\maketitle

\section{Optimal linear transform}

We update our state $x\sim N(\mu, \mat{P})$ using observation
$y = \mat{H} \mu + \epsilon$, where $\epsilon \sim N(0, \mat{R})$,
$\dim{x} = N_x$, and $\dim{y} = N_y$.
We will perform this update by first transforming our variables using
linear transformations $\mat{T}_x$ and $\mat{T}_y$ for $x$ and $y$ respectively
such that $\mat{T}_x x = \tilde{x} \sim N(\tilde{\mu}, \mat{I})$ and
$\mat{T}_y y = \tilde{y} = \mat{\Sigma} \tilde{\mu} + \tilde{\epsilon}$, where
$\mat{\Sigma}$ is diagonal, and $\tilde{\epsilon} \sim N(0, \mat{I})$.

Take,
\[
  x' = \mat{P}^{-1/2} x
  \text{ and }
  y' = \mat{R}^{-1/2} y.
\]
We then have,
\[
  y = \mat{H} \mu + \epsilon
\]
\[
  \mat{R}^{1/2} y' = \mat{H}
  \mat{P}^{1/2} \mu' + \epsilon
\]
\[
  y' = \mat{R}^{-1/2}\mat{H}
  \mat{P}^{1/2} \mu' + \epsilon',
\]
where $\epsilon \sim N(0, \mat{I})$.
We can then take the singular value decomposition of
$\mat{R}^{-1/2} \mat{H} \mat{P}^{1/2}$ yielding,
\[
  y' = \mat{U} \mat{\Sigma} \mat{V}^{T} \mu' + \epsilon'
\]
\[
  \mat{U}^T y' = \mat{\Sigma} (\mat{V}^T \mu') + \tilde{\epsilon}
\]
\[
  \tilde{y}= \mat{\Sigma} \tilde{\mu} + \tilde{\epsilon}
\]
where $\mat{\Sigma}$ is diagonal and $\tilde{\epsilon} \sim N(0,
\mat{I})$.
We have $\mat{T}_x = \mat{V}^T \mat{P}^{-1/2}$ and
$\mat{T}_y = \mat{U}^T \mat{R}^{-1/2}$.





\section{Dimension reduction optimal linear transform}

We again update our state $x \sim N(\mu, \mat{P})$ with
observations $y = \mat{H} x + \epsilon$, where $\epsilon \sim N(0,
\mat{R})$.
We will now calculate
\[
  \mat{P} = \mat{Q}_x \mat{\Lambda}_x \mat{Q}_x^T
  \text{ and }
  \mat{R} = \mat{Q}_y \mat{\Lambda}_y \mat{Q}_y^T
\]
and choose $N_{\lambda_x}$ and $N_{\lambda_y}$ that are the number of
eigenvalues to keep for $P$ and $R$ respectively such that,
\[
  \dim(\mat{Q}_x) = (N_x, N_{\lambda_x})\text{; }\dim(\mat{\Lambda}_x) =
  (N_{\lambda_x}, N_{\lambda_x})
\]
\[
  \text{ and }
\]
\[
  \dim(\mat{Q}_y) = (N_y, N_{\lambda_y})\text{; }\dim(\mat{\Lambda}_y) =
  (N_{\lambda_y}, N_{\lambda_y})
\]

We can then repeat the above calculations while reducing the
transformed variables' dimensions.
Take,
\[
  x' = \mat{\Lambda}_x^{-1/2} \mat{Q}_x^T x
  \text{ and }
  y' = \mat{\Lambda}_y^{-1/2} \mat{Q}_y^T y
\]
Note that we have,
\[
  x \approx \mat{Q}_x \mat{\Lambda}_x^{1/2} x'
\]
\[
  y \approx \mat{Q}_y \mat{\Lambda}_y^{1/2} y'
\]
where there is equality when $N_{\lambda_x}$ ($N_{\lambda_y}$) is the
true rank of $\mat{P}$ ($\mat{R}$).

Assuming that $N_{\lambda_x}$ and $N_{\lambda_y}$ are the ranks of $P$
and $R$ respectively, we have,
\[
  y = \mat{H} \mu + \epsilon
\]
\[
  \mat{Q}_y \mat{\Lambda}_y^{1/2} y' = \mat{H}
  \mat{Q}_x \mat{\Lambda}_x^{1/2} \mu' + \epsilon
\]
\[
  y' = \mat{\Lambda}_y^{-1/2} \mat{Q}_y^T \mat{H}
  \mat{Q}_x \mat{\Lambda}_x^{1/2} \mu' + \epsilon',
\]
where $\epsilon' \sim N(0, \mat{I}_{N_{\lambda_y}})$.
We can then take the singular value decomposition of
$\mat{\Lambda}_y^{-1/2} \mat{Q}_y^T \mat{H} \mat{Q}_x
\mat{\Lambda}_x^{1/2}$ yielding,
\[
  y' = \mat{U} \mat{\Sigma} \mat{V}^{T} \mu' + \epsilon'
\]
\[
  \mat{U}^T y' = \mat{\Sigma} (\mat{V}^T \mu') + \tilde{\epsilon}
\]
\[
  \tilde{y}= \mat{\Sigma} \tilde{\mu} + \tilde{\epsilon}
\]
where $\mat{\Sigma}$ is diagonal and $\tilde{\epsilon} \sim N(0,
\mat{I}_{N_{\lambda_y}})$.
We have $\mat{T}_x = \mat{V}^T \mat{\Lambda}_x^{-1/2} \mat{Q}_x^T$ and
$\mat{T}_y = \mat{U}^T \mat{\Lambda}_y^{-1/2} \mat{Q}_y^T$.
Note that we have,
\[
  \mat{T}_x \mat{T}_x^{-R} = \mat{V}^T \mat{\Lambda}_x^{-1/2}
  \mat{Q}_x^T \mat{Q}_x \mat{\Lambda}_x^{1/2} \mat{V} = \mat{I}_{N_{\lambda_x}},
\]
but there is no left inverse.
However, the right inverse is something like a left inverse in that
\[
  \mat{T}_x^{-R} \mat{T}_x = \mat{Q}_x \mat{\Lambda}_x^{1/2} \mat{V}
  \mat{V}^T \mat{\Lambda}_x^{-1/2} \mat{Q}_x^T
\]
\[
  \mat{T}_x^{-R} \mat{T}_x = \mat{Q}_x \mat{Q}_x^T
\]
which is equal to the identity matrix only if $P$ is full rank, and
all eigenvectors are kept.
However, if $\mu$ is in the span of the columns of $\mat{Q}_x$, then
we have,
\[
  \mat{T}_x^{-R} \mat{T}_x \mu = \mat{Q}_x \mat{Q}_x^T \mu = \mu
\]
even if $\mat{T}_x$ does not have a left inverse.
Furthermore, if $\mu$ is not in the span of the columns of
$\mat{Q}_x$, this is possible even if the range of $\mat{P}$ and
$\mat{Q}_x$ are equal, then we have,
\[
  (\mat{T}_x^{-R} \mat{T}_x + \mat{I}_{N_x} - \mat{Q}_x \mat{Q}_x^T)
  \mu = \mat{Q}_x \mat{Q}_x^T \mu  + \mu - \mat{Q}_x \mat{Q}_x^T \mu =
  \mu.
\]
We can therefore always recover the parts of $\mu$ that are removed
because of the representation of $\mu$ in the column space of $\mat{Q}_x$.

\section{Multi-scale optimal transform}

Suppose that we have two ensembles representing the state $x$.
One ensemble, $\mat{X}_c$ will have many members, but will have a lower
resolution and only represent large scale structures of the state.
The other ensemble, $\mat{X}$ will have fewer members, but will
have a finer resolution and therefore be able to represent fine scale
structures of the state.
We then have,
\[
  \dim \mat{X}_c = N_{x_c}, N_{e_c}
\]
\[\text{and}\]
\[
  \dim \mat{X} = N_x, N_e
\]
where $N_{x_c} < N_x$ and $N_{e_c} > N_e$.

Taking the singular value decomposition of the ensemble of
coarse perturbations, $\mat{X}_c^*$, will give us the eigenvalues and
eigenvectors of the sample covariance matrix derived from $\mat{X}_c$.
\[
  \mat{X}_c^{*} = \left( \mat{X}_c - \frac{1}{N}\bar{\mat{X}}_c
  \mat{1}_c \right) / \sqrt{N_{e_c} - 1}
\]
\[
  \mat{Q}_c \mat{\Lambda}_c^{1/2} \mat{V}^T_{temp} = \mat{X}_c^*
\]
\[
  \mat{Q}_c \mat{\Lambda}_c \mat{Q}_c^T = (\mat{X}_c^*)
  {(\mat{X}_c^*)}^{T}
\]
where $\mat{1}_c$ is a matrix of all ones of with the same dimension as
$\mat{X}_c$ and $\bar{\mat{X}}_c$ is the sample mean of $\mat{X}_c$.
Assuming that we know $\mat{Q}_y$ and $\mat{\Lambda}_y$, we can then
calculate $\mat{U}_c$ and $\mat{V}_c$,
\[
  \mat{U}_c \mat{\Sigma}_c \mat{V}_c^T = \mat{\Lambda}^{-1/2}_y
  \mat{Q}_y^T \mat{H} \mat{Q}_c \mat{\Lambda}^{1/2}_c.
\]
This gives us the optimal linear transform, or as much of it as we
deem appropriate to keep, for the coarse scale system.
We can then use this to calculate the coarse parts of the optimal
linear transform for the fine scale system.
To do this, we can use $\mat{Q}_c$, $\mat{U}_c$, and $\mat{V}_c$, but
must recalculate $\mat{\Lambda}_c$ and $\mat{\Sigma}_c$ for their fine
state equivalents $\mat{\Lambda}_{fc}$ and $\mat{\Sigma}_{fc}$.

In order to calculate $\mat{\Lambda}_{fc}$ we must first interpolate
the eigenvectors $\mat{Q}_c$ to the fine state space resulting in
$\mat{Q}_{fc}$.
The interpolated eigenvectors will need to be modified so that they
form a orthonormal set in fine space.
Once this is done, we can then calculate their corresponding
eigenvalues by assuming that the interpolated eigenvectors are left
singular values of the ensemble perturbations for $\mat{X}$,
$\mat{X}^*$:
\[
  \lambda_{fc}[i]^{1/2} = \lVert q_{fc}[i]^T \mat{X}^* \rVert
\]
where $\lambda_{fc}[i]$ is the eigenvalue of $\mat{X}^*\left(
  \mat{X}^* \right)^T$ corresponding to the eigenvector $q_{fc}[i]$.
We can then be arrange all of the eigenvalues and vectors into the
matrices $\mat{\Lambda}_{fc}$ and $\mat{Q}_{fc}$.
We can calculate the singular values by assuming that $\mat{U}_c$
and $\mat{V}_c$ have columns made of left and right singular vectors
of $\mat{\Lambda}_y^{1/2} \mat{Q}_y^T \mat{H} \mat{Q}_{fc}
\mat{\Lambda}_{fc}^{-1/2}$:
\[
  \sigma_{fc}[i] = u_c[i]^T \mat{\Lambda}_y^{1/2}
\mat{Q}_y^T \mat{H} \mat{Q}_{fc} \mat{\Lambda}_{fc}^{-1/2} v_{c}[i]
\]
where $\sigma_{fc}[i]$ is the singular value corresponding to the left
and right singular vectors $u_c[i]$ and $v_c[i]$.
These singular values can then be arranged in the matrix
$\mat{\Sigma}_{fc}$

Now that these quantities have been calculated, we can then form the
optimal linear transforms and their right inverses for our fine system
corresponding to the large scales:
\[
  \mat{T}_{xl} = \mat{V}_c^T \mat{\Lambda}_{cf}^{-1/2} \mat{Q}_{cf}^T
\]
\[
  \mat{T}_{xl}^{-R} = \mat{Q}_{cf} \mat{\Lambda}_{cf}^{1/2} \mat{V}_c
\]
\[
  \mat{T}_{yl} = \mat{U}_c^T \mat{\Lambda}_{y}^{-1/2} \mat{Q}_{y}^T
\]
\[
  \mat{T}_{yl}^{-R} = \mat{Q}_{y} \mat{\Lambda}_{y}^{1/2} \mat{U}_c
\]

We can also calculate the linear transform in our fine system
corresponding to our short scales.
Since the above transforms will assimilate the information
corresponding to the long scales, we are able to apply localization
that is appropriate for our short scales when calculating the
following transforms.
This process is very similar to the standard reduced rank optimal
linear transform, accept that we must ensure that certain vectors are
orthogonal to those used above.

We start by calculating and localizing the sample covariance based on
our fine ensemble $\mat{X}$:
\[
  \mat{P}_{loc} = \mat{L_s} \circ \mat{X}^* \left( \mat{X}^* \right)^T
\]



\section{Equivalence to standard KF}

Sticking with the above notation, we can calculate the standard Kalman
filter update and the corresponding equations:
\[
  \mat{K} = \mat{P}\mat{H}^T (\mat{H} \mat{P} \mat{H}^T + \mat{R})^{-1},
\]
\[
  \mu^a = \mu + \mat{K} (y - \mat{H} \mu),
\]
\[
  \mat{P}^a = (\mat{I} - \mat{K} \mat{H}) \mat{P}.
\]
Alternatively, in terms of our transformed equations, we have:
\[
  \tilde{\mat{K}} = \mat{I}_{N_{\lambda_x}} \mat{\Sigma}^T ( \mat{\Sigma} \mat{I}_{N_{\lambda_x}}
  \mat{\Sigma}^T + \mat{I}_{N_{\lambda_y}})^{-1} = \mat{\Sigma}^T (\mat{\Sigma}
  \mat{\Sigma}^T + \mat{I}_{N_{\lambda_y}})^{-1}
\]
\[
  \tilde{\mu}^a = \tilde{\mu} + \tilde{\mat{K}}(\tilde{y} + \mat{\Sigma} \tilde{\mu})
\]
\[
  \tilde{\mat{P}}^a = (\mat{I}_{N_{\lambda_x}} - \tilde{\mat{K}} \mat{\Sigma}) \tilde{\mat{P}}.
\]
We can then convert $\tilde{x}^a$ and $\tilde{\mat{P}}^a$ back to the
original $x$-space through,
\[
  \mu^a_1 = \mat{T}_x^{-R} \tilde{\mu}^a
\]
\[
  \text{ and }
\]
\[
  \mat{P}^a_1 = \mat{T}_x^{-R} \tilde{\mat{P}}^a \left(\mat{T}_x^{-R}\right)^T.
\]

Assuming that the ranks of $\mat{P}$ and $\mat{R}$ are $N_{\lambda_x}$
and $N_{\lambda_y}$ respectively,
\[
  \tilde{\mat{K}} = \mat{\Sigma}^T (\mat{\Sigma} \mat{\Sigma}^T +
  \mat{I}_{N_{\lambda_y}})^{-1}
\]
\[
  \tilde{\mat{K}} = \mat{T}_x \mat{P} \mat{T}_x^T (\mat{T}_y \mat{H}
  \mat{T}_x^{-R})^T \left((\mat{T}_y \mat{H} \mat{T}_x^{-R})(\mat{T}_y
  \mat{H} \mat{T}_x^{-R})^T + \mat{T}_y \mat{R} \mat{T}_y^T \right)^{-1}
\]
\[
  \tilde{\mat{K}} = \mat{T}_x \mat{P} \mat{T}_x^T \left( \mat{T}_x^{-R}
  \right)^T \mat{H}^T \mat{T}_y^T
  \left( \mat{T}_y \mat{H} \mat{T}_x^{-R} \left( \mat{T}_x^{-R}
    \right)^T \mat{H}^T \mat{T}_y^T + \mat{T}_y \mat{R} \mat{T}_y^T \right)^{-1}
\]
\[
  \tilde{\mat{K}} = \mat{T}_x \mat{P} \mat{Q}_x \mat{Q}_x^T \mat{H}^T \mat{T}_y^T
  \left( \mat{T}_y \left( \mat{H} \mat{Q}_x \Lambda_x \mat{Q}_x^T
      \mat{H}^T + \mat{R}\right) \mat{T}_y^T \right)^{-1}
\]
If we assume that $\mu$ is in the span of $\mat{Q}_x$, we have,
\[
   x^a_1 = \mat{T}_x^{-R} \tilde{x}^a
\]
\[
   x^a_1 = \mat{T}_x^{-R} \left( \tilde{x} + \tilde{K} \right)
\]


% \section{Transformation from coarse}



% We have two ensembles representing the same system.
% The ensemble $X$ is a coarse representation of the state, but has many ensemble members.
% The ensemble $Z$ is a fine representation of the sates, but has few ensemble members.
% \[
% \dim{X} = N_x, N_{ex}
% \]
% \[
% \dim{Z} = N_z, N_{ez}
% \]

% In order to find the eigenvalues and eigenvectors of the sample correlation of $X$, we take the singular value decomposition of $\tilde{X} = {(N_{ex} - 1)}^{-1/2} (X - \bar{X})/ \sd(X)$, where $\sd(X)$ is the standard deviation of each element of $X$.
% \[
% U_x S_x V_x^T = \tilde{X}
% \]
% This means that,
% \[
% U_x S_z^2 U_x^T = \hat{C}_x
% \]
% where $\hat{C}_x$ is the sample correlation of $X$.

% We want to then use $U_x$, or its leading columns, to estimate the leading $U_z$.
% To do this, we interpolate $U_x$ to the $z$ space and then use QR factorization to ensure the interpolated $u_x$'s are orthonormal:
% \[
% U_{x} = \interp (U_x)
% \]
% \[
% U_{xi}, R = \QR (U_{xi}).
% \]
% We then must choose how many of the columns of $U_{xi}$ should be used.
% The best way to make this choice is unclear.
% A few ways that I am considering are:
% \begin{enumerate}
% \item Keep $U_{xi}$ based on the cumulative sum of their corresponding eigenvalues.
% \item Keep $U_{xi}$ based on the rate of change of their corresponding eigenvalues.
% \item Keep $U_{xi}$ based on some measure of the length scale produced by the low rank approximation of $\hat{C}_x$ and the residual.
% \item Keep $U_{xi}$ based on how orthogonal the interpolated vector is in $z$ space before orthogonalization.
% \end{enumerate}
% It should be noted that keeping all columns can be detrimental to the assimilation process.
% If some of the columns of $U_{xi}$ are representing small scale structures, then in the following steps they will still be used to represent some of these small scale structures.

% After choosing which columns of $U_{xi}$ to keep, we must determine what eigenvalues they should have.
% We do this by taking $\lambda = u_{xi}^T \hat{C}_z u_{xi}$ as the eigenvalue of $u{xi}$ for our approximation of $C_z$.
% Alternatively, we can take $\lambda = {(\tilde{Z}^T u_{xi})}^T (\tilde{Z}^T u_{xi})$ where $\tilde{Z}$ is defined similarly as $\tilde{X}$.
% This will give us the leading eigenvectors and an approximation of their eigenvalues of $C_Z$.

% These eigenvectors and eigenvalues represent the large scale structure of the problem.
% To then find the small scales of the problem, we take
% \[
% C_z^\bot = C_z^\parallel - \hat{C}_z
% \]
% where
% \[
% C_z^\parallel = U_{xi} \Lambda_{xi}  U_{xi}^T
% \]
% and $\Lambda_{xi}$ is the diagonal matrix with the $\lambda$s described above on the diagonal.
% We can then localize $C_z^\bot$ to get at the small scales that are represented in $Z$.

% We must choose how to localize $C_z^\bot$.
% One reasonable expectation is that the scales in $C_z^\bot$ will be shorter or similar to $N_z/N_x$.
% This choice will also be affected by the choice of how many columns of $U_{xi}$ to keep.
% Once the localization matrix $L$ is chosen, we can then generate our localized correlation matrix as:
% \[
% C_z^{loc} = C_z^\parallel + L \circ C_Z^\bot
% \]
% and the corresponding covariance matrix as:
% \[
% P_z^{loc} = D_z C_z^{loc} D_z.
% \]
% where $D_z$ is the diagonal matrix with sample standard deviations of $Z$ on the diagonal.

% We can then find the leading eigenvectors and eigenvalues of $C_z^{loc}$:
% \[
% Q \Lambda Q^T = C_z^{loc}
% \]
% and use them to transform the $z$ variable.
% First, we must calculate a whitening transformation
% \[
% T_w = \Lambda^{-1/2} Q^T
% \]
% and its right inverse
% \[
% T_w^i = Q \Lambda^{1/2}
% \]
% and the singular value decomposition
% \[
% U \Sigma V^T = R^{-1/2} H T_w^i.
% \]
% We can then define transformations for both $z$ and $y$
% \[
% T_z = V^T T_w
% \]
% \[
% T_y = U^T R^{-1/2}
% \]

% We then know $z^* = T_z z$ and $y^* = T_y y$ have identity covariance matrices and
% \[
% y^* = \Sigma z^* + \epsilon^*
% \]
% \[
% U^T R^{-1/2} y = \Sigma V^T \Lambda^{-1/2} Q^T z + \epsilon^*
% \]
% \[
% y = R^{1/2} U \Sigma V^T \Lambda^{-1/2} Q^T z  + \epsilon
% \]
% \[
% y = R^{1/2}R^{-1/2} H Q \Lambda^{1/2} \Lambda^{-1/2} Q^T z  + \epsilon
% \]
% \[
% y = H z  + \epsilon
% \]
% The question is: can I do this better?
% Specifically:
% \begin{enumerate}
% \item Can I calculate the eigenvectors and values of $C^{loc}$ without generating $C^\parallel$ or $C^\bot$ explicitly?
% \item Can I calculate the eigenvectors and values of $P$ from those of $C$ without generating $C$?
% \item Can I calculate the singular vectors and values of $R^{-1/2} H T_w^i$ from the singular value decomposition of $R$, $H$, and $T_w^i$?
% \end{enumerate}




















\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
